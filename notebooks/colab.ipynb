{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "colab.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCwe_IkyvC1H",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Torch2vec - Text similarity on Steroids\n",
        "---\n",
        "This notebook is about a PyTorch implementation of Doc2Vec (distributed memory) with similarity measure.\n",
        "\n",
        "I will be using ArXiV's papers abstracts to compute similarity between abstracts.\n",
        "\n",
        "Now let's dive in...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M9ySbEiu2f5",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Activate GPU support\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "juv6LZDJswyn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9ed78ad3-f02f-4af6-f2a6-ee919dd3969b"
      },
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBzj4ICqvyl9",
        "colab_type": "text"
      },
      "source": [
        "If successful, the output of the cell above should print `True`. Note that Google Colaboratory also offers [TPU](https://cloud.google.com/tpu/) support. These *Tensor Processing Units* are specifically designed for machine learning tasks and may outperform conventional GPUs. While support for TPUs in PyTorch is still pending, [tensorflow](https://www.tensorflow.org/) models may benefit from using TPUs (see [this tutorial](https://colab.research.google.com/notebooks/tpu.ipynb)).\n",
        "\n",
        "### Useful commands\n",
        "\n",
        "Within the notebook environment, you can not only execute Python code, but also bash commands by prepending a `!`. For example, you can install new Python packages via the package manager `pip`. Here, we just check the installed version of PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olw1LkikszeB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip show torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inyd1Hz5v--N",
        "colab_type": "text"
      },
      "source": [
        "Another useful command is `!kill -9 -1`. It will reset all running kernels and free up memory (including GPU memory). Furthermore, there are a few commands to have a closer look on the hardware spcifications, i.e. to get information about the installed CPU and GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXDoIgghwFcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!lscpu |grep 'Model name'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqPPI7R8wIVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pD7szzdbwOuj",
        "colab_type": "text"
      },
      "source": [
        "In addition, you can check the available RAM and HDD memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Go2xIJL6wK0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cat /proc/meminfo | grep 'MemAvailable'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6JNQP-ewRqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!df -h / | awk '{print $4}'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg_CgBeCwfkD",
        "colab_type": "text"
      },
      "source": [
        "Finally, one can execute the following command to get a live update on the GPU \n",
        "\n",
        "---\n",
        "\n",
        "usage. This is useful to check how much of the GPU memory is in use to optimize \n",
        "\n",
        "---\n",
        "\n",
        "the batchsize for training. Note that whenever the training routine in a notebook is still running, you need to execute this command in another Colaboratory notebook to get an instant response:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaDw2g8DwjnL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_awfBOtwp3P",
        "colab_type": "text"
      },
      "source": [
        "### Mount Google Drive\n",
        "Another important prerequisite for training our neural network is a place to save checkpoints of the trained model and to store obtained training data. Colaboratory provides convenient access to Google Drive via the `google.colab` Python module. The following command will mount your Google Drive contents to the folder path `/content/gdrive` on the Colaboratory instance. For authentication, you have to click the generated link and paste the authorization code into the input field:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oXPd5U0wsJd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKfjibRywyZs",
        "colab_type": "text"
      },
      "source": [
        "### Download the arxiv meta data with gsutil\n",
        "We will need gsutil utility from google cloud sdk. Firstly, you need to authenticate yourself in Colab. Once you run the code below, it will ask you to follow a link to login and enter an access token that you receive upon successful login.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVCz-5Zrw4pa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBl99qzFxApr",
        "colab_type": "text"
      },
      "source": [
        "We would be using the gsutil command to upload and download files. So we first need to install the GCloud SDK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FniU-d2zw8hs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!curl https://sdk.cloud.google.com | bash\n",
        "!gcloud init"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWnSEg_LwWI7",
        "colab_type": "text"
      },
      "source": [
        "### Download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PCDmsnMwrc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!gsutil cp -n gs://arxiv-dataset/metadata-v5/arxiv-metadata-oai.json /content/gdrive/My\\ Drive/arxiv-metadata-oai.json\n",
        "!ls -l /content/gdrive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e0RcJn1xXqA",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Reading the entire json metadata\n",
        "This cell may take a minute to run considering the volume of data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJFIlZSTxZlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tqdm\n",
        "import json\n",
        "\n",
        "input_file = \"/content/gdrive/My Drive/arxiv-metadata-oai.json\"\n",
        "\n",
        "data  = []\n",
        "with tqdm.tqdm(total=os.path.getsize(input_file)) as pbar:\n",
        "     with open(input_file, 'r') as f:\n",
        "          for line in f:\n",
        "              pbar.update(len(line))\n",
        "              data.append(json.loads(line))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rv3xiC8Hxq_j",
        "colab_type": "text"
      },
      "source": [
        "I'm limiting my analysis to just 50,000 documents because of the compute limit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CpMusIPxr_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "train = pd.DataFrame(train[:50000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "G9xf0bHAswzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shortuuid\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Ni6HTJkyswzX",
        "colab_type": "code",
        "colab": {},
        "outputId": "df9b4eda-7f4b-40fe-883e-a6db3c2837c3"
      },
      "source": [
        "for i in tqdm.tqdm(range(len(train))):\n",
        "    train.loc[i,'id']=shortuuid.uuid()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 197465/197465 [00:36<00:00, 5425.06it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TMgd2yROsw0L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus = train['authors'].fillna('')+' '+train['title'].fillna('')+' '+train['summary'].fillna('')+' '+train['subjects'].fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Frm-eWCrsw0S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus.index = train['id']\n",
        "corpus.name = 'text'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Hj4MB45xsw0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "corpus.to_csv('corpus.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wY9phlpBsw0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "del corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "3nuSZNZasw0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import tqdm\n",
        "from time import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from numpy.random import choice\n",
        "from torchtext.data import RawField,Field, TabularDataset\n",
        "from spacy.lang.en import STOP_WORDS\n",
        "import string\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class DataPreparation():\n",
        "    def __init__(self,corpus_path,vocab_size=None):\n",
        "        data = pd.read_csv(corpus_path)\n",
        "        self.corpus = data.iloc[:,1]\n",
        "        self.document_ids = data.iloc[:,0].values\n",
        "#         self.window_size = window_size\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.vocab_size = vocab_size if vocab_size else None\n",
        "        \n",
        "        \n",
        "    def vocab_builder(self):\n",
        "        tqdm.tqdm.pandas(desc='--- Tokenizing ---')\n",
        "        self.corpus = self.corpus.progress_apply(self._tokenize_str)\n",
        "        vocab = [word for sentence in self.corpus.values for word in sentence]\n",
        "        word_counts = Counter(vocab)\n",
        "        if not self.vocab_size:\n",
        "            self.vocab_size = len(vocab)\n",
        "        self.word_counts = word_counts.most_common()[:self.vocab_size]\n",
        "        self.vocab = [word[0] for word in self.word_counts]+['[UNK]']\n",
        "        self.vocab_size = len(self.vocab)\n",
        "        self.word_id_mapper = {word:ids for ids,word in enumerate(self.vocab)}\n",
        "        self.id_word_mapper = dict(zip(self.word_id_mapper.values(),self.word_id_mapper.keys()))\n",
        "            \n",
        "    \n",
        "    def _tokenize_str(self,str_):\n",
        "        stopwords = list(STOP_WORDS)+list((''.join(string.punctuation)).strip(''))+['-pron-','-PRON-']\n",
        "        # keep only alphanumeric and punctations\n",
        "        str_ = re.sub(r'[^A-Za-z0-9(),.!?\\'`]', ' ', str_)\n",
        "        # remove multiple whitespace characters\n",
        "        str_ = re.sub(r'\\s{2,}', ' ', str_)\n",
        "        # punctations to tokens\n",
        "        str_ = re.sub(r'\\(', ' ( ', str_)\n",
        "        str_ = re.sub(r'\\)', ' ) ', str_)\n",
        "        str_ = re.sub(r',', ' , ', str_)\n",
        "        str_ = re.sub(r'\\.', ' . ', str_)\n",
        "        str_ = re.sub(r'!', ' ! ', str_)\n",
        "        str_ = re.sub(r'\\?', ' ? ', str_)\n",
        "        # split contractions into multiple tokens\n",
        "        str_ = re.sub(r'\\'s', ' \\'s', str_)\n",
        "        str_ = re.sub(r'\\'ve', ' \\'ve', str_)\n",
        "        str_ = re.sub(r'n\\'t', ' n\\'t', str_)\n",
        "        str_ = re.sub(r'\\'re', ' \\'re', str_)\n",
        "        str_ = re.sub(r'\\'d', ' \\'d', str_)\n",
        "        str_ = re.sub(r'\\'ll', ' \\'ll', str_)\n",
        "        # lower case\n",
        "\n",
        "        return [word for word in str_.strip().lower().split() if word not in stopwords and len(word)>2]\n",
        "    \n",
        "    def get_data(self,window_size,num_noise_words):\n",
        "        '''\n",
        "        num_noise_words: number of words to be negative sampled\n",
        "        '''\n",
        "        self._padder(window_size)\n",
        "        data = self._corpus_to_num()\n",
        "        instances = self._instance_count(window_size)\n",
        "        context = np.zeros((instances,window_size*2+1),dtype=np.int32)\n",
        "        doc = np.zeros((instances,1),dtype=np.int32)\n",
        "        k = 0 \n",
        "        for doc_id, sentence  in (enumerate(tqdm.tqdm(data,desc='---- Creating Data ----'))):\n",
        "            for i in range(window_size, len(sentence)-window_size):\n",
        "                context[k] = sentence[i-window_size:i+window_size+1] # Get surrounding words\n",
        "                doc[k] = doc_id\n",
        "                k += 1\n",
        "                \n",
        "        target = context[:,window_size]\n",
        "        context = np.delete(context,window_size,1)\n",
        "        doc = doc.reshape(-1,)\n",
        "        target_noise_ids = self._sample_noise_distribution(num_noise_words,window_size)\n",
        "        target_noise_ids = np.insert(target_noise_ids,0,target,axis=1)\n",
        "        \n",
        "        \n",
        "        context = torch.from_numpy(context).type(torch.LongTensor)\n",
        "        doc = torch.from_numpy(doc).type(torch.LongTensor)\n",
        "        target_noise_ids = torch.from_numpy(target_noise_ids).type(torch.LongTensor)\n",
        "        \n",
        "#         context = torch.from_numpy(context).type(torch.LongTensor).to(self.device)\n",
        "#         doc = torch.from_numpy(doc).type(torch.LongTensor).to(self.device)\n",
        "#         target_noise_ids = torch.from_numpy(target_noise_ids).type(torch.LongTensor).to(self.device)\n",
        "        \n",
        "        return doc,context,target_noise_ids\n",
        "            \n",
        "    def _padder(self,window_size):\n",
        "        for i in range(len(self.corpus.values)):\n",
        "            self.corpus.values[i] = ('[UNK] '*window_size).strip().split()+self.corpus.values[i]+('[UNK] '*window_size).strip().split()\n",
        "            \n",
        "    def _corpus_to_num(self):\n",
        "        num_corpus = []\n",
        "        unk_count = 0\n",
        "        for sentence in self.corpus.values:\n",
        "            sen = []\n",
        "            for word in sentence:\n",
        "                if word in self.word_id_mapper:\n",
        "                    sen.append(self.word_id_mapper[word])\n",
        "                else:\n",
        "                    sen.append(self.word_id_mapper['[UNK]'])\n",
        "                    unk_count+=1\n",
        "            num_corpus.append(sen)\n",
        "            \n",
        "        self.word_counts+=[('[UNK]',unk_count)]\n",
        "        return np.array(num_corpus)\n",
        "    \n",
        "    def _instance_count(self,window_size):\n",
        "        instances = 0\n",
        "        for i in self.corpus.values:\n",
        "            instances+=len(i)-2*window_size   \n",
        "        return instances\n",
        "        \n",
        "    def _sample_noise_distribution(self,num_noise_words,window_size):\n",
        "        \n",
        "        probs = np.zeros(self.vocab_size)\n",
        "\n",
        "        for word, freq in self.word_counts:\n",
        "            probs[self.word_id_mapper[word]] = freq\n",
        "\n",
        "        probs = np.power(probs, 0.75)\n",
        "        probs /= np.sum(probs)\n",
        "\n",
        "        return choice(probs.shape[0],(self._instance_count(window_size),num_noise_words),p=probs).astype(np.int32)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "D_mDDzbtsw0q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = DataPreparation('corpus.csv') #if going out of memory when using pytorch model then you can restrict model size by using vocab_size argument"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "YAeBGQ5rsw0t",
        "colab_type": "code",
        "colab": {},
        "outputId": "98c9e7a4-8d85-422a-e545-e9b2d6e705a4"
      },
      "source": [
        "data.vocab_builder()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
            "  from pandas import Panel\n",
            "--- Tokenizing ---: 100%|██████████| 197465/197465 [03:22<00:00, 973.69it/s] \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yHDl4Eqwsw0z",
        "colab_type": "code",
        "colab": {},
        "outputId": "5c36c9cf-9198-4886-9712-42d1100abf43"
      },
      "source": [
        "doc, context, target_noise_ids = data.get_data(window_size=3,num_noise_words=6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---- Creating Data ----: 100%|██████████| 197465/197465 [01:14<00:00, 2667.17it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EVI9IwXtsw02",
        "colab_type": "code",
        "colab": {},
        "outputId": "518013c3-3615-4eb0-e2ee-4a4c1f57df1b"
      },
      "source": [
        "len(doc)/1000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20673.196"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GvTeplOnsw06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,doc_ids,context, target_noise_ids):\n",
        "        self.doc_ids = doc_ids\n",
        "        self.context = context\n",
        "        self.target_noise_ids = target_noise_ids\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.doc_ids)\n",
        "    \n",
        "    def __getitem__(self,index):\n",
        "        return self.doc_ids[index], self.context[index], self.target_noise_ids[index]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ldTxgECEsw0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NegativeSampling(nn.Module):\n",
        "    \n",
        "    \n",
        "    def __init__(self):\n",
        "        super(NegativeSampling, self).__init__()\n",
        "        self._log_sigmoid = nn.LogSigmoid()\n",
        "\n",
        "    def forward(self, scores):\n",
        "        \n",
        "        k = scores.size()[1] - 1\n",
        "        return -torch.sum(\n",
        "            self._log_sigmoid(scores[:, 0])\n",
        "            + torch.sum(self._log_sigmoid(-scores[:, 1:]), dim=1) / k\n",
        "        ) / scores.size()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jD5U8k4Isw1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class DM(nn.Module):\n",
        "    \"\"\"Distributed Memory version of Paragraph Vectors.\n",
        "    Parameters\n",
        "    ----------\n",
        "    vec_dim: int\n",
        "        Dimensionality of vectors to be learned (for paragraphs and words).\n",
        "    num_docs: int\n",
        "        Number of documents in a dataset.\n",
        "    num_words: int\n",
        "        Number of distinct words in a daset (i.e. vocabulary size).\n",
        "    \"\"\"\n",
        "    def __init__(self, vec_dim, num_docs, num_words):\n",
        "        super(DM, self).__init__()\n",
        "        # paragraph matrix\n",
        "        self._D = nn.Parameter(\n",
        "            torch.randn(num_docs, vec_dim), requires_grad=True)\n",
        "        # word matrix\n",
        "        self._W = nn.Parameter(\n",
        "            torch.randn(num_words, vec_dim), requires_grad=True)\n",
        "        # output layer parameters\n",
        "        self._O = nn.Parameter(\n",
        "            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n",
        "\n",
        "    def forward(self, context_ids, doc_ids, target_noise_ids):\n",
        "        \n",
        "        \n",
        "        # combine a paragraph vector with word vectors of\n",
        "        # input (context) words\n",
        "        x = torch.add(\n",
        "            self._D[doc_ids, :], torch.sum(self._W[context_ids, :], dim=1))\n",
        "\n",
        "        # sparse computation of scores (unnormalized log probabilities)\n",
        "        # for negative sampling\n",
        "        return torch.bmm(\n",
        "            x.unsqueeze(1),\n",
        "            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n",
        "\n",
        "    def get_paragraph_vector(self):\n",
        "        return self._D.data.tolist()\n",
        "    \n",
        "    def fit(self,doc_ids,context,target_noise_ids,epochs,batch_size,num_workers=1):\n",
        "        \n",
        "        opt=torch.optim.Adam(self.parameters(),lr=0.0001)\n",
        "        cost_func = NegativeSampling()\n",
        "        if torch.cuda.is_available():            \n",
        "            cost_func.cuda()\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        dataset = Dataset(doc_ids, context, target_noise_ids)\n",
        "        dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,num_workers=num_workers)\n",
        "        loss = []\n",
        "        for epoch in range(epochs):\n",
        "            step = 0\n",
        "            pbar = tqdm.tqdm(dataloader,desc='Epoch= {} ---- prev loss={}'.format(epoch+1,loss))\n",
        "            loss=[]\n",
        "            \n",
        "            for doc_ids,context_ids,target_noise_ids in pbar:\n",
        "                doc_ids = doc_ids.to(device)\n",
        "                context_ids = context_ids.to(device)\n",
        "                target_noise_ids = target_noise_ids.to(device)\n",
        "                x = self.forward(\n",
        "                        context_ids,\n",
        "                        doc_ids,\n",
        "                        target_noise_ids) \n",
        "                x = cost_func.forward(x)\n",
        "                loss.append(x.item())\n",
        "                self.zero_grad()\n",
        "                x.backward()\n",
        "                opt.step()\n",
        "#                 if step%100==0:\n",
        "#                     print('-',end='')\n",
        "            loss = torch.mean(torch.FloatTensor(loss))\n",
        "#             print('epoch - {} loss - {:.4f}'.format(epoch+1,loss))\n",
        "        tqdm.tqdm.write('Final loss: {:.4f}'.format(loss))\n",
        "        \n",
        "    def save_model(self,ids,file_name):\n",
        "        docvecs = self._D.data.cpu().numpy()\n",
        "        if len(docvecs)!=len(ids):\n",
        "            raise(\"Length of ids does'nt match\")\n",
        "            \n",
        "            \n",
        "        self.embeddings = np.concatenate([ids.reshape(-1,1),docvecs],axis=1)\n",
        "        np.save(file_name,self.embeddings,fix_imports=False)\n",
        "        \n",
        "    def load_model(self,file_path):\n",
        "        self.embeddings = np.load(file_path,allow_pickle=True,fix_imports=False)\n",
        "        \n",
        "    \n",
        "    def similar_docs(self,docs,topk=10):\n",
        "        topk=topk+1\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        if not isinstance(docs,np.ndarray):\n",
        "            docs = np.array(docs)\n",
        "        \n",
        "        docids = self.embeddings[:,0]\n",
        "        vecs = self.embeddings[:,1:]\n",
        "        mask = np.isin(docids,docs)\n",
        "        if not mask.any():\n",
        "            raise('Not in vocab')\n",
        "            \n",
        "        given_docvecs = torch.FloatTensor(vecs[mask].tolist()).to(device)\n",
        "        vecs = torch.FloatTensor(vecs.tolist()).to(device)\n",
        "        similars = self._similarity(given_docvecs,vecs,topk)\n",
        "        similar_docs = docids[similars.indices.tolist()[0]].tolist()\n",
        "        probs = similars.values.tolist()[0]\n",
        "        \n",
        "        return similar_docs[1:], probs[1:]\n",
        "        \n",
        "    def _similarity(self,doc,embeddings,topk):\n",
        "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        similarity = []\n",
        "        \n",
        "        cos=nn.CosineSimilarity(dim=0).to(device)\n",
        "        for i in doc:\n",
        "            inner = []\n",
        "            for j in embeddings:\n",
        "                inner.append(cos(i.view(-1,1),j.view(-1,1)).tolist())\n",
        "            similarity.append(inner)\n",
        "        similarity = torch.FloatTensor(similarity).view(1,-1).to(device)\n",
        "        return torch.topk(similarity,topk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ty83UEt3sw1H",
        "colab_type": "code",
        "colab": {},
        "outputId": "5d33a0fb-a726-4e6e-de5d-8438b786a194"
      },
      "source": [
        "data.vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "771909"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "y9cRRKdTsw1Q",
        "colab_type": "code",
        "colab": {},
        "outputId": "64a3a54d-4681-45e9-edbb-fc2ee5085d1f"
      },
      "source": [
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "197465"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "EpvfNwJPsw1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = DM(vec_dim=100,num_docs=len(data),num_words=data.vocab_size).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "eNUxpQ9ksw1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_workers=os.cpu_count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "60QLcWKesw1j",
        "colab_type": "code",
        "colab": {},
        "outputId": "b33741b9-580c-445e-f9f4-f3f0b1f738d4"
      },
      "source": [
        "num_workers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8IZh3jdPsw1n",
        "colab_type": "code",
        "colab": {},
        "outputId": "65a1f047-2092-4d63-c638-884bd1864e53"
      },
      "source": [
        "model.fit(doc,context,target_noise_ids,epochs=1,batch_size=3000,num_workers=num_workers) #epochs can be increased set to be 1 for testing purpose"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch= 1 ---- prev loss=[]: 100%|██████████| 6892/6892 [09:41<00:00, 11.85it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Final loss: 1.1714\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RTj6d3qFsw1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_model(data.document_ids,'weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "M8A7pMBasw1v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_model('weights.npy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "VG9_eFfNsw17",
        "colab_type": "code",
        "colab": {},
        "outputId": "203c37d3-f54a-4ead-c28d-ed5449a9a9a6"
      },
      "source": [
        "np.load('weights.npy',allow_pickle=True).nbytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159551720"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8eugIdhwsw2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.similar_docs('E2HayXNpNnFfDd5U7LUX2o',topk=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "adBb7DjGsw2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}