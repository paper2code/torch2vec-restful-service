{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/recsys/suggest_dump.txt\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('../input/recsys/suggest_dump.txt',delimiter='\\t')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['id']=1","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shortuuid\nimport tqdm","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in tqdm.tqdm(range(len(train))):\n    train.loc[i,'id']=shortuuid.uuid()","execution_count":5,"outputs":[{"output_type":"stream","text":"100%|██████████| 197465/197465 [00:36<00:00, 5425.06it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus = train['authors'].fillna('')+' '+train['title'].fillna('')+' '+train['summary'].fillna('')+' '+train['subjects'].fillna('')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus.index = train['id']\ncorpus.name = 'text'","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus.to_csv('corpus.csv')","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del corpus","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import re\nimport tqdm\nfrom time import time\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom numpy.random import choice\nfrom torchtext.data import RawField,Field, TabularDataset\nfrom spacy.lang.en import STOP_WORDS\nimport string\nfrom collections import Counter\n\n\nclass DataPreparation():\n    def __init__(self,corpus_path,vocab_size=None):\n        data = pd.read_csv(corpus_path)\n        self.corpus = data.iloc[:,1]\n        self.document_ids = data.iloc[:,0].values\n#         self.window_size = window_size\n        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        self.vocab_size = vocab_size if vocab_size else None\n        \n        \n    def vocab_builder(self):\n        tqdm.tqdm.pandas(desc='--- Tokenizing ---')\n        self.corpus = self.corpus.progress_apply(self._tokenize_str)\n        vocab = [word for sentence in self.corpus.values for word in sentence]\n        word_counts = Counter(vocab)\n        if not self.vocab_size:\n            self.vocab_size = len(vocab)\n        self.word_counts = word_counts.most_common()[:self.vocab_size]\n        self.vocab = [word[0] for word in self.word_counts]+['[UNK]']\n        self.vocab_size = len(self.vocab)\n        self.word_id_mapper = {word:ids for ids,word in enumerate(self.vocab)}\n        self.id_word_mapper = dict(zip(self.word_id_mapper.values(),self.word_id_mapper.keys()))\n            \n    \n    def _tokenize_str(self,str_):\n        stopwords = list(STOP_WORDS)+list((''.join(string.punctuation)).strip(''))+['-pron-','-PRON-']\n        # keep only alphanumeric and punctations\n        str_ = re.sub(r'[^A-Za-z0-9(),.!?\\'`]', ' ', str_)\n        # remove multiple whitespace characters\n        str_ = re.sub(r'\\s{2,}', ' ', str_)\n        # punctations to tokens\n        str_ = re.sub(r'\\(', ' ( ', str_)\n        str_ = re.sub(r'\\)', ' ) ', str_)\n        str_ = re.sub(r',', ' , ', str_)\n        str_ = re.sub(r'\\.', ' . ', str_)\n        str_ = re.sub(r'!', ' ! ', str_)\n        str_ = re.sub(r'\\?', ' ? ', str_)\n        # split contractions into multiple tokens\n        str_ = re.sub(r'\\'s', ' \\'s', str_)\n        str_ = re.sub(r'\\'ve', ' \\'ve', str_)\n        str_ = re.sub(r'n\\'t', ' n\\'t', str_)\n        str_ = re.sub(r'\\'re', ' \\'re', str_)\n        str_ = re.sub(r'\\'d', ' \\'d', str_)\n        str_ = re.sub(r'\\'ll', ' \\'ll', str_)\n        # lower case\n\n        return [word for word in str_.strip().lower().split() if word not in stopwords and len(word)>2]\n    \n    def get_data(self,window_size,num_noise_words):\n        '''\n        num_noise_words: number of words to be negative sampled\n        '''\n        self._padder(window_size)\n        data = self._corpus_to_num()\n        instances = self._instance_count(window_size)\n        context = np.zeros((instances,window_size*2+1),dtype=np.int32)\n        doc = np.zeros((instances,1),dtype=np.int32)\n        k = 0 \n        for doc_id, sentence  in (enumerate(tqdm.tqdm(data,desc='---- Creating Data ----'))):\n            for i in range(window_size, len(sentence)-window_size):\n                context[k] = sentence[i-window_size:i+window_size+1] # Get surrounding words\n                doc[k] = doc_id\n                k += 1\n                \n        target = context[:,window_size]\n        context = np.delete(context,window_size,1)\n        doc = doc.reshape(-1,)\n        target_noise_ids = self._sample_noise_distribution(num_noise_words,window_size)\n        target_noise_ids = np.insert(target_noise_ids,0,target,axis=1)\n        \n        \n        context = torch.from_numpy(context).type(torch.LongTensor)\n        doc = torch.from_numpy(doc).type(torch.LongTensor)\n        target_noise_ids = torch.from_numpy(target_noise_ids).type(torch.LongTensor)\n        \n#         context = torch.from_numpy(context).type(torch.LongTensor).to(self.device)\n#         doc = torch.from_numpy(doc).type(torch.LongTensor).to(self.device)\n#         target_noise_ids = torch.from_numpy(target_noise_ids).type(torch.LongTensor).to(self.device)\n        \n        return doc,context,target_noise_ids\n            \n    def _padder(self,window_size):\n        for i in range(len(self.corpus.values)):\n            self.corpus.values[i] = ('[UNK] '*window_size).strip().split()+self.corpus.values[i]+('[UNK] '*window_size).strip().split()\n            \n    def _corpus_to_num(self):\n        num_corpus = []\n        unk_count = 0\n        for sentence in self.corpus.values:\n            sen = []\n            for word in sentence:\n                if word in self.word_id_mapper:\n                    sen.append(self.word_id_mapper[word])\n                else:\n                    sen.append(self.word_id_mapper['[UNK]'])\n                    unk_count+=1\n            num_corpus.append(sen)\n            \n        self.word_counts+=[('[UNK]',unk_count)]\n        return np.array(num_corpus)\n    \n    def _instance_count(self,window_size):\n        instances = 0\n        for i in self.corpus.values:\n            instances+=len(i)-2*window_size   \n        return instances\n        \n    def _sample_noise_distribution(self,num_noise_words,window_size):\n        \n        probs = np.zeros(self.vocab_size)\n\n        for word, freq in self.word_counts:\n            probs[self.word_id_mapper[word]] = freq\n\n        probs = np.power(probs, 0.75)\n        probs /= np.sum(probs)\n\n        return choice(probs.shape[0],(self._instance_count(window_size),num_noise_words),p=probs).astype(np.int32)\n    \n    def __len__(self):\n        return len(self.corpus)\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = DataPreparation('corpus.csv') #if going out of memory when using pytorch model then you can restrict model size by using vocab_size argument","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.vocab_builder()","execution_count":12,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/tqdm/std.py:666: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n  from pandas import Panel\n--- Tokenizing ---: 100%|██████████| 197465/197465 [03:22<00:00, 973.69it/s] \n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"doc, context, target_noise_ids = data.get_data(window_size=3,num_noise_words=6)","execution_count":13,"outputs":[{"output_type":"stream","text":"---- Creating Data ----: 100%|██████████| 197465/197465 [01:14<00:00, 2667.17it/s]\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(doc)/1000","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"20673.196"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self,doc_ids,context, target_noise_ids):\n        self.doc_ids = doc_ids\n        self.context = context\n        self.target_noise_ids = target_noise_ids\n        \n    def __len__(self):\n        return len(self.doc_ids)\n    \n    def __getitem__(self,index):\n        return self.doc_ids[index], self.context[index], self.target_noise_ids[index]","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class NegativeSampling(nn.Module):\n    \n    \n    def __init__(self):\n        super(NegativeSampling, self).__init__()\n        self._log_sigmoid = nn.LogSigmoid()\n\n    def forward(self, scores):\n        \n        k = scores.size()[1] - 1\n        return -torch.sum(\n            self._log_sigmoid(scores[:, 0])\n            + torch.sum(self._log_sigmoid(-scores[:, 1:]), dim=1) / k\n        ) / scores.size()[0]","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass DM(nn.Module):\n    \"\"\"Distributed Memory version of Paragraph Vectors.\n    Parameters\n    ----------\n    vec_dim: int\n        Dimensionality of vectors to be learned (for paragraphs and words).\n    num_docs: int\n        Number of documents in a dataset.\n    num_words: int\n        Number of distinct words in a daset (i.e. vocabulary size).\n    \"\"\"\n    def __init__(self, vec_dim, num_docs, num_words):\n        super(DM, self).__init__()\n        # paragraph matrix\n        self._D = nn.Parameter(\n            torch.randn(num_docs, vec_dim), requires_grad=True)\n        # word matrix\n        self._W = nn.Parameter(\n            torch.randn(num_words, vec_dim), requires_grad=True)\n        # output layer parameters\n        self._O = nn.Parameter(\n            torch.FloatTensor(vec_dim, num_words).zero_(), requires_grad=True)\n\n    def forward(self, context_ids, doc_ids, target_noise_ids):\n        \n        \n        # combine a paragraph vector with word vectors of\n        # input (context) words\n        x = torch.add(\n            self._D[doc_ids, :], torch.sum(self._W[context_ids, :], dim=1))\n\n        # sparse computation of scores (unnormalized log probabilities)\n        # for negative sampling\n        return torch.bmm(\n            x.unsqueeze(1),\n            self._O[:, target_noise_ids].permute(1, 0, 2)).squeeze()\n\n    def get_paragraph_vector(self):\n        return self._D.data.tolist()\n    \n    def fit(self,doc_ids,context,target_noise_ids,epochs,batch_size,num_workers=1):\n        \n        opt=torch.optim.Adam(self.parameters(),lr=0.0001)\n        cost_func = NegativeSampling()\n        if torch.cuda.is_available():            \n            cost_func.cuda()\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        dataset = Dataset(doc_ids, context, target_noise_ids)\n        dataloader = torch.utils.data.DataLoader(dataset,batch_size=batch_size,num_workers=num_workers)\n        loss = []\n        for epoch in range(epochs):\n            step = 0\n            pbar = tqdm.tqdm(dataloader,desc='Epoch= {} ---- prev loss={}'.format(epoch+1,loss))\n            loss=[]\n            \n            for doc_ids,context_ids,target_noise_ids in pbar:\n                doc_ids = doc_ids.to(device)\n                context_ids = context_ids.to(device)\n                target_noise_ids = target_noise_ids.to(device)\n                x = self.forward(\n                        context_ids,\n                        doc_ids,\n                        target_noise_ids) \n                x = cost_func.forward(x)\n                loss.append(x.item())\n                self.zero_grad()\n                x.backward()\n                opt.step()\n#                 if step%100==0:\n#                     print('-',end='')\n            loss = torch.mean(torch.FloatTensor(loss))\n#             print('epoch - {} loss - {:.4f}'.format(epoch+1,loss))\n        tqdm.tqdm.write('Final loss: {:.4f}'.format(loss))\n        \n    def save_model(self,ids,file_name):\n        docvecs = self._D.data.cpu().numpy()\n        if len(docvecs)!=len(ids):\n            raise(\"Length of ids does'nt match\")\n            \n            \n        self.embeddings = np.concatenate([ids.reshape(-1,1),docvecs],axis=1)\n        np.save(file_name,self.embeddings,fix_imports=False)\n        \n    def load_model(self,file_path):\n        self.embeddings = np.load(file_path,allow_pickle=True,fix_imports=False)\n        \n    \n    def similar_docs(self,docs,topk=10):\n        topk=topk+1\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        if not isinstance(docs,np.ndarray):\n            docs = np.array(docs)\n        \n        docids = self.embeddings[:,0]\n        vecs = self.embeddings[:,1:]\n        mask = np.isin(docids,docs)\n        if not mask.any():\n            raise('Not in vocab')\n            \n        given_docvecs = torch.FloatTensor(vecs[mask].tolist()).to(device)\n        vecs = torch.FloatTensor(vecs.tolist()).to(device)\n        similars = self._similarity(given_docvecs,vecs,topk)\n        similar_docs = docids[similars.indices.tolist()[0]].tolist()\n        probs = similars.values.tolist()[0]\n        \n        return similar_docs[1:], probs[1:]\n        \n    def _similarity(self,doc,embeddings,topk):\n        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n        similarity = []\n        \n        cos=nn.CosineSimilarity(dim=0).to(device)\n        for i in doc:\n            inner = []\n            for j in embeddings:\n                inner.append(cos(i.view(-1,1),j.view(-1,1)).tolist())\n            similarity.append(inner)\n        similarity = torch.FloatTensor(similarity).view(1,-1).to(device)\n        return torch.topk(similarity,topk)","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.vocab_size","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"771909"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(data)","execution_count":19,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"197465"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DM(vec_dim=100,num_docs=len(data),num_words=data.vocab_size).cuda()","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_workers=os.cpu_count()","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_workers","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"2"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(doc,context,target_noise_ids,epochs=1,batch_size=3000,num_workers=num_workers) #epochs can be increased set to be 1 for testing purpose","execution_count":23,"outputs":[{"output_type":"stream","text":"Epoch= 1 ---- prev loss=[]: 100%|██████████| 6892/6892 [09:41<00:00, 11.85it/s]","name":"stderr"},{"output_type":"stream","text":"Final loss: 1.1714\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save_model(data.document_ids,'weights')","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_model('weights.npy')","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.load('weights.npy',allow_pickle=True).nbytes","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"159551720"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.similar_docs('E2HayXNpNnFfDd5U7LUX2o',topk=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}